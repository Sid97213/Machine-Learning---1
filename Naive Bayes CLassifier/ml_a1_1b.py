# -*- coding: utf-8 -*-
"""ML_A1_1B

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vj9ATJUzu5bCtjlPcv_HYxmB0d3LKWSf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import math
np.random.seed(4)

from google.colab import files
files.upload()

# reading dataset
data = pd.read_csv('dataset_NB.txt',sep='\t', header=None)
data = data.sample(frac = 1)
data = list(data[0])

puncs = [',','.','!',';','"','$','&','(',')','[',']']
numbers = ['0','1','2','3','4','5','6','7','8','9']

# k for k-fold cross validation
k = 7
# filter for length of words
maxwlen = 3

# grps holds the k groups we divide out entire data into
def make_groups(dtset, k):
    grps = []
    total = len(dtset)
    # dividing the entire dataset into k groups
    jump = math.ceil(total/k)
    fr = 0
    to = fr+jump
    while(to < total):
        grps.append(list(data[fr:to]))
        fr = to
        to += jump

    if fr != total+1:
        grps.append(data[fr:total])
    
    return grps

# filtering out words that contain punctuation symbols or numerics in between them
# filtering out words less than 'maxlen' characters long 
def filter_words(split, maxlen=3):
    filtered = []
    for w in split:
        w = w.lower()
        flag = 0
        for i in range(len(w)):
            if (w[i] in puncs) or (w[i] in numbers):
                flag = 1
                break
        if flag:
            flag = 0
            continue
        if len(w) < maxlen:
            continue
        filtered.append(w)
    return filtered

# get the training and testing data as required for k-fold cross-validation
# ind represents index of the group that is to be chosen for testing
def get_train_test(groups, ind, mlen):
    # print(len(groups))
    train_data = []
    test_data = groups[ind]
    for i in range(len(groups)):
        if i != ind:
            train_data += groups[i]
    # words are first split according to regular expression rules
    train_sentences = [filter_words(list(set(re.split(r"\W+",train_data[i][:-1]))),mlen) for i in range(len(train_data))]
    train_labels = [int(train_data[i][-1]) for i in range(len(train_data))]
    test_sentences = [filter_words(list(set(re.split(r"\W+",test_data[i][:-1]))),mlen) for i in range(len(test_data))]
    test_labels = [int(test_data[i][-1]) for i in range(len(test_data))]
    
    return train_sentences, train_labels, test_sentences, test_labels

# building the vocabulary 'dictionary' containing words that occur in 
# the entire training data
def get_vocab(train_s, train_l):
    vocab0 = {}
    vocab1 = {}
    for i in range(len(train_s)):
        filtered = train_s[i]
        for f in filtered:
            # if present in vocabulary for 0 already
            if (train_l[i] == 0):
                if (f in vocab0.keys()):
                    vocab0[f] += 1
                else:
                    vocab0[f] = 1
            # if present in vocabulary for 1 already
            if (train_l[i] == 1):
                if (f in vocab1.keys()):
                    vocab1[f] += 1
                else:
                    vocab1[f] = 1

    vocab = {0: vocab0, 1: vocab1}
    return vocab

# Laplace smoothing to handle words missing from either class
def laplace_smoothing(vocab):
    for w in vocab[0].keys():
        if w not in vocab[1].keys():
            vocab[1][w] = 1
        else:
            vocab[1][w] += 1

    for w in vocab[1].keys():
        if w not in vocab[0].keys():
            vocab[0][w] = 1
        else:
            vocab[0][w] += 1
    
    return vocab

def runNB(dtset,k,maxlen):
    avg = 0
    grps = make_groups(dtset,k)
    print("Minimum word length: {}".format(maxlen))
    for i in range(k):
        training_s, training_l, testing_s, testing_l = get_train_test(grps, i, maxlen)
        vocabulary = get_vocab(training_s, training_l)
        # total number of words in class 0 and class 1
        w_zero = sum(vocabulary[0].values())
        w_one = sum(vocabulary[1].values())
        
        vocabulary = laplace_smoothing(vocabulary)

        # examples of class 0, class 1
        ones = sum(training_l)
        zeros = len(training_l) - ones

        total = zeros + ones
        pred = []
        for ts in testing_s:
            prob0 = math.log(zeros/total,10)
            prob1 = math.log(ones/total,10)
            for w in ts:
                if (w not in vocabulary[0].keys()) and (w not in vocabulary[1].keys()):
                    continue
                prob0 += math.log(vocabulary[0][w]/(zeros+2),10)
                prob1 += math.log(vocabulary[1][w]/(ones+2),10)
            if prob0 < prob1:
                pred.append(1)
            else:
                pred.append(0)
    
        matches = 0
        for j in range(len(pred)):
            if (pred[j] == testing_l[j]):
                matches += 1
    
        success = 100*matches/len(testing_s)
        
        print("Accuracy in Fold {}: {}%".format(i,round(success,2)))
        avg += success

    avg = avg/k
    print("Average classification accuracy: {}%".format(round(avg,2)))

runNB(data, k, 1)

runNB(data, k, 2)

runNB(data, k, 3)

runNB(data, k, 4)

